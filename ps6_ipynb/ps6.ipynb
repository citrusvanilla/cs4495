{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PS 6: Particle Tracking\n",
    "\n",
    "#### Description:\n",
    "\n",
    "Recall that we need a variety of elements:\n",
    "1. model - this is the “thing” that is actually being tracked. Maybe it’s a patch, a contour, or some other description of an entity\n",
    "2. a representation of state xt that describes the state of the model at time t;\n",
    "3. a dynamics model p(xt | xt-1)  that describes the distribution of the state at time t given the state at t − 1;\n",
    "4. a measurement zt that some how captures the data of the current image; and finally,\n",
    "5. a sensor model p(zt | xt) that gives the likelihood of a measurement given the state. For Bayesian-based tracking, we assume that at time t − 1 we have a belief about the state represented as a density p(xt-1),  and that given some measurements zt at time t we update our Belief by computing the posterior density:\n",
    "Bel(xt) ∝ p(zt | xt)p(xt | ut, xt-1)Bel(xt-1)\n",
    "\n",
    "The Kalman filter provided an analytic method for doing this under the assumption that density representing the belief at time t, the noise component of the dynamics and the sensor model were all simple Gaussian distributions. Particle filters provide a sample-based method of representing densities that removes that restriction and also is tolerant of occasional large deviations from the well-behaved model. In this assignment, you will be implementing a particle filter to track entities in video.\n",
    "\n",
    "Three video (.avi) files are provided in the input directory:\n",
    "- pres_debate.avi, which shows two candidates in a 2012 town hall debate,\n",
    "- noisy_debate.avi, which is the same video overlaid with fluctuating Gaussian noise, and,\n",
    "- pedestrians.avi, which shows a group of people crossing a street in London.\n",
    "\n",
    "For each video file, there is a text file that contains the object bounding box at the first frame that you can use for initialization. \n",
    "The format of the text file is:\n",
    "- x y\n",
    "- w h\n",
    "\n",
    "where (x, y) is the top-left coordinate (not center), and (w, h) is the size (width, height) of the bounding box.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Notes: Algorithm Sketch and Parameters\n",
    "\n",
    "1.  Start off with a randomized set of particles and normalization constant of zero (constant weights).  The particles in our example represent the center of an image patch that we are attempting to track through time.\n",
    "\n",
    "Then, for each particle in the new sample:\n",
    "\n",
    "2. Choose one sample with a probabilty defined by its weight from previous time step.\n",
    "3. Assume the state in the current time is a function of the state in the previous time period and some dynamics model (in our case, just xy Gaussian noise). Obtain a measurement (in our case, the MSE difference between the template of the face and the template centered about the sample). Using this measurement, obtain a probability of obtaining such a measurement and reweight the sample accordingly.\n",
    "4. Place the sample and its weight back into a new set of particles.\n",
    "5. Once the specified number of samples is obtained in the above manner, adjust all weights in the new set of particles so that they sum to 1.\n",
    "6. Resample from the new set, and return to the loop starting at step (2).\n",
    "\n",
    "For this specific problem set:\n",
    "\n",
    "1.  The \"Model\" we are tracking is an image patch.\n",
    "2.  The \"State\" is xy-position of this window in the scene.\n",
    "3.  The \"Dynamics model\" is Gaussian noise in xy coordinate system.\n",
    "4.  Our \"Measurement\" is the MSE between the sampled state and the previous or base state.\n",
    "5.  Our \"Sensor model\" returns a probability of observing our new state given the previous belief about the state, based on 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1. Particle Filter Tracking - The Presidential Debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries and header\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# file and directory information\n",
    "os.chdir('/Users/justinfung/Desktop/udacity/cv/hw/ps6_python/input')\n",
    "movie1 = 'pres_debate.mov'\n",
    "movie2 = 'noisy_debate.mov'\n",
    "\n",
    "# parameters for the particle filter\n",
    "num_particles = 200\n",
    "sigma_MSE = 5\n",
    "sigma_Gauss = 10\n",
    "alpha = 0\n",
    "\n",
    "# initial detection window, where (u,v,,) is the top-left coordinate (not center), \n",
    "# and (,,m,n) is the size (width, height) of the bounding box.\n",
    "template = (320,175,103,129) # problem 1.a\n",
    "#template = (350,210,40,60) # problem 1.b\n",
    "#template = (300,150,140,180) # problem 1.b\n",
    "\n",
    "# start a videowriter object for output\n",
    "# from: http://www.pyimagesearch.com/2016/02/22/writing-to-video-with-opencv/\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output.mov',fourcc, 30.0, (1280,720))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Methods\n",
    "\n",
    "In order to visualize the tracker’s behavior you will need to overlay each successive frame with the following visualizations:\n",
    "- Every particle’s (u,v) location in the distribution should be plotted by drawing a colored dot point on the image. Remember that this should be the center of the window, not the corner.\n",
    "- Draw the rectangle of the tracking window associated with the Bayesian estimate for the current location which is simply the weighted mean of the (u,v) of the particles.\n",
    "- Finally we need to get some sense of the standard deviation or spread of the distribution. First, find the distance of every particle to the weighted mean. Next, take the weighted sum of these distances and plot a circle centered at the weighted mean with this radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_centerpoints(img,num_points,pointsarr,radius,color,offsetx,offsety):\n",
    "    \"\"\"\n",
    "    Draws a colored dot representing every particle’s (u,v) location in the distribution. \n",
    "    This dot represents the center of the window, not the corner.\n",
    "    ----\n",
    "    img: frame from video on which to overlay particles as points\n",
    "    num_points: number of points to draw, should be same as num of particles\n",
    "    pointsarr: 2D array of points (shape ex: (2,200)) in which first column is row, \n",
    "               2nd is col; points indicate TOP LEFT extent of window\n",
    "    radius: radius of the visualized point\n",
    "    color: color in BGR as a tuple\n",
    "    offsetx: distance to center of detection window from left\n",
    "    offsety: distnce to center of detection window from top\n",
    "    \"\"\"\n",
    "    for i in xrange(num_points):\n",
    "        cv2.circle(img,\n",
    "                   (pointsarr[1][i]+offsetx,pointsarr[0][i]+offsety),\n",
    "                   radius, \n",
    "                   color, \n",
    "                   -1)\n",
    "\n",
    "def draw_window(img, topleftpoint, radius, color, offsetx, offsety):\n",
    "    \"\"\"\n",
    "    Draws the rectangle of the tracking window associated with the Bayesian estimate \n",
    "    for the current location which is simply the weighted mean of the (u,v) of the particles.\n",
    "    -----\n",
    "    img: frame from video on which to overlay particles as points\n",
    "    topleftpoint: xy val of top left point of window as a tuple\n",
    "    radius: radius of the visualized window\n",
    "    color: color in BGR as a tuple\n",
    "    offsetx: distance to center of detection window from left\n",
    "    offsety: distnce to center of detection window from top\n",
    "    \"\"\"    \n",
    "    cv2.rectangle(img,\n",
    "                  (topleftpoint[0],topleftpoint[1]),\n",
    "                  (topleftpoint[0]+offsetx*2, topleftpoint[1]+offsety*2),\n",
    "                  color,\n",
    "                  3)\n",
    "\n",
    "def draw_circle(img, pointsarr, weighted_mean, weights, color, offsetx, offsety):\n",
    "    \"\"\"\n",
    "    Draws a circle centered at the weighted mean of the particles with a radius that\n",
    "    represents the standard deviation or spread of the distribution.\n",
    "    -----\n",
    "    img: frame from video on which to overlay particles as points\n",
    "    pointsarr: 2D array of points (shape ex: (2,200)) in which first column is row, \n",
    "               2nd is col; points indicate TOP LEFT extent of window\n",
    "    weighted_mean: weight_mean as tuple of (x,y)\n",
    "    weights: weights of the distribution of particles\n",
    "    color: color in BGR as a tuple\n",
    "    offsetx: distance to center of detection window from left\n",
    "    offsety: distnce to center of detection window from top\n",
    "    \"\"\"\n",
    "    distances = np.zeros(pointsarr.shape[1])\n",
    "    i = 0\n",
    "    for i in xrange(distances.shape[0]):\n",
    "        distances[i] = ((pointsarr[1][i]-weighted_mean[0])**2 + (pointsarr[0][i]-weighted_mean[1])**2) ^ (1/2)\n",
    "    weighted_sum = int(np.dot(distances,weights)/100)\n",
    "    \n",
    "    cv2.circle(img,\n",
    "               (weighted_mean[0] + offsetx, weighted_mean[1] + offsety), \n",
    "               weighted_sum, \n",
    "               (0,0,255), \n",
    "               -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Particle Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in video and get meta data\n",
    "# take first non-empty frame of the video\n",
    "cap = cv2.VideoCapture(movie2)\n",
    "start = 0\n",
    "ret, initframe = cap.read()\n",
    "while initframe.mean() == 0:\n",
    "    ret, initframe = cap.read()\n",
    "    start += 1\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "\n",
    "# initialize arrays to hold particles and normalization constants\n",
    "uv = np.zeros((num_frames*2*num_particles),dtype=np.int).reshape(num_frames,2,num_particles)\n",
    "eta = np.zeros((num_frames*num_particles),dtype=np.float).reshape(num_frames,num_particles)\n",
    "\n",
    "# setup initial location of window and the (1) state,\n",
    "# using the following notation:\n",
    "u = template[0] # u is top left x\n",
    "v = template[1] # v is top left y\n",
    "m = template[2] # m is width of window (columns)\n",
    "n = template[3] # n is height of window (rows)\n",
    "u_p = u + m/2  # u_p is center x\n",
    "v_p = v + n/2  # v_p is center y\n",
    "template_patch = initframe[v:v+n,u:u+m,:] # img[rows,cols,channels]\n",
    "template_center = initframe[v_p,u_p,:]\n",
    "\n",
    "# initialize top-left particle coordinates and weights\n",
    "uv[0][0] = np.random.randint(v-n/2,v+n/2,size = num_particles) # random y-coordinates (row)\n",
    "uv[0][1] = np.random.randint(u-m/2,u+m/2,size = num_particles) # random x-coordinates (column)\n",
    "eta[0] = np.ones(num_particles) / num_particles # normalized constant weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 frames complete.\n",
      "20 frames complete.\n",
      "30 frames complete.\n",
      "40 frames complete.\n",
      "50 frames complete.\n",
      "60 frames complete.\n",
      "70 frames complete.\n",
      "80 frames complete.\n",
      "90 frames complete.\n",
      "100 frames complete.\n",
      "110 frames complete.\n",
      "120 frames complete.\n",
      "130 frames complete.\n",
      "140 frames complete.\n",
      "150 frames complete.\n",
      "160 frames complete.\n",
      "170 frames complete.\n",
      "180 frames complete.\n",
      "190 frames complete.\n",
      "200 frames complete.\n",
      "210 frames complete.\n",
      "220 frames complete.\n",
      "230 frames complete.\n",
      "240 frames complete.\n",
      "250 frames complete.\n",
      "260 frames complete.\n",
      "270 frames complete.\n",
      "280 frames complete.\n",
      "290 frames complete.\n",
      "300 frames complete.\n",
      "310 frames complete.\n",
      "320 frames complete.\n",
      "330 frames complete.\n",
      "340 frames complete.\n",
      "350 frames complete.\n",
      "360 frames complete.\n",
      "370 frames complete.\n",
      "380 frames complete.\n",
      "390 frames complete.\n",
      "400 frames complete.\n",
      "410 frames complete.\n",
      "420 frames complete.\n",
      "430 frames complete.\n",
      "440 frames complete.\n",
      "450 frames complete.\n",
      "460 frames complete.\n",
      "470 frames complete.\n",
      "480 frames complete.\n",
      "490 frames complete.\n",
      "500 frames complete.\n",
      "510 frames complete.\n",
      "520 frames complete.\n",
      "530 frames complete.\n",
      "540 frames complete.\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "# -> loop through all the frames\n",
    "for fr in xrange(start,num_frames):\n",
    "    ret,frame = cap.read()\n",
    "    eta_sum = 0\n",
    "    \n",
    "    for particle in xrange(num_particles):\n",
    "        # resample from previous particle states\n",
    "        s_idx = np.random.choice(num_particles, 1, p=eta[fr-1]).item() #random choice by weight\n",
    "        \n",
    "        # dynamics model from (3)        \n",
    "        uv[fr][0][particle] = uv[fr-1][0][s_idx] + np.round(np.random.normal(0,sigma_Gauss))\n",
    "        uv[fr][1][particle] = uv[fr-1][1][s_idx] + np.round(np.random.normal(0,sigma_Gauss))\n",
    "        \n",
    "        # check for coordinates outside the extent of the frame and adjust if necessary\n",
    "        if uv[fr][1][particle] < 0:\n",
    "            uv[fr][1][particle] = 0\n",
    "        elif uv[fr][1][particle] > frame_width:\n",
    "            uv[fr][1][particle] = frame_width\n",
    "        \n",
    "        if uv[fr][0][particle] < 0:\n",
    "            uv[fr][0][particle] = 0\n",
    "        elif uv[fr][0][particle] > frame_height:\n",
    "            uv[fr][0][particle] = frame_height\n",
    "        \n",
    "        # measurement from (4)\n",
    "        u_p = uv[fr][1][particle] + m/2; #center x-coor/col\n",
    "        v_p = uv[fr][0][particle] + n/2; #center y-coor/row\n",
    "        image_patch = frame[uv[fr][0][particle]:uv[fr][0][particle] + n,\n",
    "                            uv[fr][1][particle]:uv[fr][1][particle] + m, :]\n",
    "        particleMSE = 1. / (m*n) * np.sum((template_patch - image_patch)**2)\n",
    "\n",
    "        # probability of observing such a state from (5)\n",
    "        eta[fr][particle] = np.exp(-(particleMSE / (2 * sigma_MSE ^ 2)))\n",
    "        \n",
    "        # track a cumsum for normalization\n",
    "        eta_sum += eta[fr][particle]\n",
    "    \n",
    "    # normalize probabilities\n",
    "    eta[fr] = eta[fr] / eta_sum\n",
    "\n",
    "    # visualizations for display\n",
    "    draw_centerpoints(frame, num_particles, uv[fr], 3, (0,255,0), m/2, n/2)\n",
    "    \n",
    "    weighted_mean_x = int(np.average(uv[fr][1], weights=eta[fr]))\n",
    "    weighted_mean_y = int(np.average(uv[fr][0], weights=eta[fr]))\n",
    "    weighted_mean = (weighted_mean_x,weighted_mean_y)\n",
    "    \n",
    "    draw_window(frame, weighted_mean, 3, (0,255,0), m/2, n/2)\n",
    "    draw_circle(frame, uv[fr], weighted_mean, eta[fr],(0,0,255), m/2, n/2)\n",
    "    \n",
    "    out.write(frame)\n",
    "    \n",
    "    # print status update\n",
    "    if fr % 10 == 0:\n",
    "        print fr , \"frames complete.\"\n",
    "        \n",
    "    # capture some frames for output\n",
    "    if fr == 28:\n",
    "        ps6_1_a_2 = frame\n",
    "    elif fr == 84:\n",
    "        ps6_1_a_3 = frame\n",
    "    elif fr == 144: \n",
    "        ps6_1_a_4 = frame\n",
    "    elif fr == 14:\n",
    "        ps6_1_e_1 = frame\n",
    "    elif fr == 32:\n",
    "        ps6_1_e_2 = frame\n",
    "    elif fr == 46:\n",
    "        ps6_1_e_3 = frame\n",
    "\n",
    "# cleanup resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Ouput\n",
    "\n",
    "#### 1A. Implement the particle filter and run it on the pres debate.avi clip. \n",
    "\n",
    "> You should begin by attempting to track Romney’s face. Tweak the parameters including window size until you can get the tracker to follow his face faithfully (5-15 pixels) up until he turns his face significantly. Run the tracker and save the video frames 28, 84, and 144 with the visualizations overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Output:\n",
    "os.chdir('/Users/justinfung/Desktop/udacity/cv/hw/ps6_python/output')\n",
    "\n",
    "#the image patch used for tracking as ps6-1-a-1.png\n",
    "cv2.imwrite(\"ps6-1-a-1.png\",template_patch)\n",
    "\n",
    "#the image frame number 28 with overlaid visualizations as ps6-1-a-2.png\n",
    "cv2.imwrite(\"ps6-1-a-2.png\",ps6_1_a_2)\n",
    "\n",
    "#the image frame number 84 with overlaid visualizations as ps6-1-a-3.png\n",
    "cv2.imwrite(\"ps6-1-a-3.png\",ps6_1_a_3)\n",
    "\n",
    "#the image frame number 144 with overlaid visualizations as ps6-1-a-4.png\n",
    "cv2.imwrite(\"ps6-1-a-4.png\",ps6_1_a_4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1B. Experiment with different dimensions for the window image patch you are trying to track. \n",
    "\n",
    ">Decrease the window size until the performance of the tracker degrades significantly. Try significantly larger windows than what worked in 1-a. Discuss the trade-offs of window size and what makes some image patches work better than others for tracking.\n",
    "\n",
    ">Describe 2-3 advantages of larger window size and 2-3 advantages of smaller window size:\n",
    "\n",
    "* __Larger Window Size__:\n",
    "  - Theoretically less susceptible to noise in the frame due to larger inference region in the measurement step.\n",
    "  - Should be more robust to suboptimal convergence due to more granularity in a larger window.\n",
    "\n",
    "\n",
    "* __Smaller Window Size__:\n",
    "  - Computationally more-efficient as computations increase in O(mn) with window diameter.\n",
    "  - Should be faster to convergence as MSE has a greater variance with less number of pixels.\n",
    "\n",
    "\n",
    "#### 1C. Adjust the σMSE parameter to higher and lower values and run the tracker.\n",
    "\n",
    ">Discuss how changing σMSE parameter alters the results and attempt to explain why.\n",
    "\n",
    "* __Larger σMSE parameter__:\n",
    "  - We increase our parameter by a factor of 2, to a value of 20.\n",
    "  - Mathematically, a higher parameter widens the spread of a Gaussian distribution, which in effect penalizes dissimilarity to a lesser degree and therefore encourages \"exploration\" of the particles in the sensor model due to higher weights being placed on outlying measurements..\n",
    "  - In our problem, this causes the tracker to drift significantly under occlusion.  However, due to the greater value of the parameter, we witness our tracker regain confident tracking when the candidate's face becomes visible again.\n",
    "  \n",
    "\n",
    "* __Smaller σMSE parameter__:\n",
    "  - We reduce our parameter by a factor of 2, to a value of 5.\n",
    "  - Mathematically, this penalizes dissimilarity to a greater degree and therefore reduces \"exploration\" of the particles in the sensor model.\n",
    "  - In our problem, this causes the tracker to faithfully stay centered on the presidential candidates face even in the case of occlusion.  The tracker appears to move very little in any given frame.\n",
    "\n",
    "\n",
    "#### 1D. Try and optimize the number of particles needed to track the target.\n",
    "\n",
    "* __Optimized particle number:__\n",
    "  - We choose the range of 100-200 particles to be \"optimal\", though \"optimality\" in our context is not an entirely measurable thing.  We consider any tracker to recover properly from a servere occlusion to be acceptable, and therefore we choose the lowest range of particle for which we observe this behavior.\n",
    "\n",
    "\n",
    "* __Discuss the trade-offs of using a larger number of particles to represent the distribution:__\n",
    "  - Computational requirements scale linearly with the addition of more particles, so all things equal, we prefer less particles.\n",
    "  - Less particles however may subject us to mischaracterization of the underlying dynamics due to undersampling- in this case we might observe our tracker drift into an unrecoverable local maximum.\n",
    "  - Larger particle numbers gives us greater confidence in our underlying dynamics and will leave us less susceptible to local maxima.\n",
    "\n",
    "\n",
    "#### 1E. Run your tracker on noisy debate.avi and see what happens. \n",
    "\n",
    ">Tune your parameters so that the cluster is able to latch back onto his face after the noise disappears. Include varying σMSE. Report how the particles respond to increasing and decreasing noise. Save the video frames 14, 32, and 46 with the visualizations overlaid.\n",
    "\n",
    "* __Discuss how you managed to tune the parameters:__\n",
    "  - The pulsing of the red circle (the indicator of spread of particles) and the spread of the green particles themselves is highly correlated with the addition of noise.  This is too be expected as the addition of random noise will have a direct negative impact on measured MSE and thus the updated weights of particles in each frame.\n",
    "  - The MSE was lowered so as not to encourage too much drift of the tracker that might be induced by the addition of noise.\n",
    "  - The same number of particles (n = 100) was utilized as in the denoised case.\n",
    "  - Under these conditions, we are still able to obtain acceptable tracking of the candidates face.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Output:\n",
    "os.chdir('/Users/justinfung/Desktop/udacity/cv/hw/ps6_python/output')\n",
    "\n",
    "# the image frame number 14 with overlaid visualizations as ps6-1-e-1.png\n",
    "cv2.imwrite(\"ps6-1-e-1.png\",ps6_1_e_1)\n",
    "\n",
    "# the image frame number 32 with overlaid visualizations as ps6-1-e-2.png\n",
    "cv2.imwrite(\"ps6-1-e-2.png\",ps6_1_e_2)\n",
    "\n",
    "# the image frame number 46 with overlaid visualizations as ps6-1-e-3.png\n",
    "cv2.imwrite(\"ps6-1-e-3.png\",ps6_1_e_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Appearance Model Update\n",
    "\n",
    "> Let’s say we’d now like to track Romney’s left hand (the one not holding the mic). You might find that it’s difficult to keep up using the naive tracker you wrote in question 1. The issue is that while making rapid hand gestures, the appearance of the hand significantly changes as it rotates and changes perspective. However, if we make the assumption that the appearance changes smoothly over time, we can update our appearance model over time.\n",
    "\n",
    ">Modify your existing tracker to include a step which uses the history to update the tracking window model. We can accomplish this using what’s called an Infinite Impulse Response (IIR) filter. The concept is simple: we first find the best tracking window for the current particle distribution as displayed in the visualizations. Then we just update the current window model to be a weighted sum of the last model and the current best estimate:\n",
    "\n",
    "> __Template(t) = (alpha) Best(t) + (1 - alpha) Template(1-t)__,\n",
    "\n",
    "> where Best(t) is the patch of the best estimate or mean estimate. It’s easy to see that by recursively updating this sum, the window implements an exponentially decaying weighted sum of (all) the past windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2A. Implement the appearance model update. \n",
    "\n",
    "> Run the tracker on pres_debate.avi and adjust parameters until you can track Romney’s hand up to frame 140. Run the tracker and save the video frames 15, 50, and 140 with the visualizations overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file and directory information\n",
    "os.chdir('/Users/justinfung/Desktop/udacity/cv/hw/ps6_python/input')\n",
    "movie1 = 'pres_debate.mov'\n",
    "\n",
    "# parameters for the particle filter\n",
    "num_particles = 200\n",
    "sigma_MSE = 20\n",
    "sigma_Gauss = 10\n",
    "alpha = 0\n",
    "\n",
    "# initial detection window, where (u,v,,) is the top-left coordinate (not center), \n",
    "# and (,,m,n) is the size (width, height) of the bounding box.\n",
    "template = (510,380,125,125) # problem 2.a\n",
    "\n",
    "# start a videowriter object for output\n",
    "# from: http://www.pyimagesearch.com/2016/02/22/writing-to-video-with-opencv/\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output_hand.mov',fourcc, 30.0, (1280,720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in video and get meta data\n",
    "# take first non-empty frame of the video\n",
    "cap = cv2.VideoCapture(movie1)\n",
    "start = 0\n",
    "ret, initframe = cap.read()\n",
    "while initframe.mean() == 0:\n",
    "    ret, initframe = cap.read()\n",
    "    start += 1\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "\n",
    "# initialize arrays to hold particles and normalization constants\n",
    "uv = np.zeros((num_frames*2*num_particles),dtype=np.int).reshape(num_frames,2,num_particles)\n",
    "eta = np.zeros((num_frames*num_particles),dtype=np.float).reshape(num_frames,num_particles)\n",
    "\n",
    "# setup initial location of window and the (1) state,\n",
    "# using the following notation:\n",
    "u = template[0] # u is top left x\n",
    "v = template[1] # v is top left y\n",
    "m = template[2] # m is width of window (columns)\n",
    "n = template[3] # n is height of window (rows)\n",
    "u_p = u + m/2  # u_p is center x\n",
    "v_p = v + n/2  # v_p is center y\n",
    "template_patch = initframe[v:v+n,u:u+m,:] # img[rows,cols,channels]\n",
    "template_center = initframe[v_p,u_p,:]\n",
    "\n",
    "# initialize top-left particle coordinates and weights\n",
    "uv[0][0] = np.random.randint(v-n/2,v+n/2,size = num_particles) # random y-coordinates (row)\n",
    "uv[0][1] = np.random.randint(u-m/2,u+m/2,size = num_particles) # random x-coordinates (column)\n",
    "eta[0] = np.ones(num_particles) / num_particles # normalized constant weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 frames complete.\n",
      "20 frames complete.\n",
      "30 frames complete.\n",
      "40 frames complete.\n",
      "50 frames complete.\n",
      "60 frames complete.\n",
      "70 frames complete.\n",
      "80 frames complete.\n",
      "90 frames complete.\n",
      "100 frames complete.\n",
      "110 frames complete.\n",
      "120 frames complete.\n",
      "130 frames complete.\n",
      "140 frames complete.\n",
      "150 frames complete.\n",
      "160 frames complete.\n",
      "170 frames complete.\n",
      "180 frames complete.\n",
      "190 frames complete.\n",
      "200 frames complete.\n",
      "210 frames complete.\n",
      "220 frames complete.\n",
      "230 frames complete.\n",
      "240 frames complete.\n",
      "250 frames complete.\n",
      "260 frames complete.\n",
      "270 frames complete.\n",
      "280 frames complete.\n",
      "290 frames complete.\n",
      "300 frames complete.\n",
      "310 frames complete.\n"
     ]
    }
   ],
   "source": [
    "# MAIN_2\n",
    "# -> loop through all the frames\n",
    "for fr in xrange(start,num_frames):\n",
    "    ret,frame = cap.read()\n",
    "    eta_sum = 0\n",
    "    \n",
    "    for particle in xrange(num_particles):\n",
    "        # resample from previous particle states\n",
    "        s_idx = np.random.choice(num_particles, 1, p=eta[fr-1]).item() #random choice by weight\n",
    "        \n",
    "        # dynamics model from (3)        \n",
    "        uv[fr][0][particle] = uv[fr-1][0][s_idx] + np.round(np.random.normal(0,sigma_Gauss))\n",
    "        uv[fr][1][particle] = uv[fr-1][1][s_idx] + np.round(np.random.normal(0,sigma_Gauss))\n",
    "        \n",
    "        # check for coordinates outside the extent of the frame and adjust if necessary\n",
    "        if uv[fr][1][particle] < 0:\n",
    "            uv[fr][1][particle] = 0\n",
    "        elif uv[fr][1][particle] > frame_width:\n",
    "            uv[fr][1][particle] = frame_width\n",
    "        \n",
    "        if uv[fr][0][particle] < 0:\n",
    "            uv[fr][0][particle] = 0\n",
    "        elif uv[fr][0][particle] > frame_height:\n",
    "            uv[fr][0][particle] = frame_height\n",
    "        \n",
    "        # measurement from (4)\n",
    "        u_p = uv[fr][1][particle] + m/2; #center x-coor/col\n",
    "        v_p = uv[fr][0][particle] + n/2; #center y-coor/row\n",
    "        image_patch = frame[uv[fr][0][particle]:uv[fr][0][particle] + n,\n",
    "                            uv[fr][1][particle]:uv[fr][1][particle] + m, :]\n",
    "        particleMSE = 1. / (m*n) * np.sum((template_patch - image_patch)**2)\n",
    "\n",
    "        # probability of observing such a state from (5)\n",
    "        eta[fr][particle] = np.exp(-(particleMSE / (2 * sigma_MSE ^ 2)))\n",
    "        \n",
    "        # track a cumsum for normalization\n",
    "        eta_sum += eta[fr][particle]\n",
    "    \n",
    "    # normalize probabilities\n",
    "    eta[fr] = eta[fr] / eta_sum\n",
    "\n",
    "    # visualizations for display\n",
    "    draw_centerpoints(frame, num_particles, uv[fr], 3, (0,255,0), m/2, n/2)\n",
    "    \n",
    "    weighted_mean_x = int(np.average(uv[fr][1], weights=eta[fr]))\n",
    "    weighted_mean_y = int(np.average(uv[fr][0], weights=eta[fr]))\n",
    "    weighted_mean = (weighted_mean_x,weighted_mean_y)\n",
    "    \n",
    "    draw_window(frame, weighted_mean, 3, (0,255,0), m/2, n/2)\n",
    "    draw_circle(frame, uv[fr], weighted_mean, eta[fr],(0,0,255), m/2, n/2)\n",
    "    \n",
    "    out.write(frame)\n",
    "    \n",
    "    # print status update\n",
    "    if fr % 10 == 0:\n",
    "        print fr , \"frames complete.\"\n",
    "        \n",
    "    # capture some frames for output\n",
    "    if fr == 28:\n",
    "        ps6_1_a_2 = frame\n",
    "    elif fr == 84:\n",
    "        ps6_1_a_3 = frame\n",
    "    elif fr == 144: \n",
    "        ps6_1_a_4 = frame\n",
    "    elif fr == 14:\n",
    "        ps6_1_e_1 = frame\n",
    "    elif fr == 32:\n",
    "        ps6_1_e_2 = frame\n",
    "    elif fr == 46:\n",
    "        ps6_1_e_3 = frame\n",
    "\n",
    "# cleanup resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
